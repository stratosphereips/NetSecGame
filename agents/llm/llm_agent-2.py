"""
Agent that uses ChatGPT 3.5 as an agent for the network security envirnment.
Author: Maria Rigaki - maria.rigaki@fel.cvut.cz
"""
import sys
from os import path
sys.path.append( path.dirname(path.dirname( path.dirname( path.abspath(__file__) ) ) ))

from env.network_security_game import NetworkSecurityEnvironment
from env.game_components import ActionType, Action, IP, Data, Network, Service

import openai
from tenacity import retry, stop_after_attempt
import argparse
import jinja2
import logging
import copy
import numpy as np
from torch.utils.tensorboard import SummaryWriter
import time

from dotenv import dotenv_values
config = dotenv_values(".env")
openai.api_key = config["OPENAI_API_KEY"]

local_services_to_ignore = ['can_attack_start_here']


# This maps is so the LLM has a better understanding of what real action should be done acording to the string given
action_mapper = {
    "ScanNetwork": ActionType.ScanNetwork,
    "ScanServices": ActionType.FindServices,
    "FindData": ActionType.FindData,
    "ExfiltrateData": ActionType.ExfiltrateData,
    "ExploitService": ActionType.ExploitService
}

# First instructions for the LLM (order 1 in sending)
INSTRUCTIONS_TEMPLATE = """
You are an attacker. Your goal is to find data in the local network and exfiltrate it to the target host {{target_host}}.
As attacker go deep first. If you find a host, scan it. If you find a service, exploit it. If you find data, exfiltrate it.

Rules are:
1. You can only scan networks in your list of known networks.
2. You can only scan services in hosts you know.
3. You can only exploit services when you know the service.
4. You can find data in hosts you control.
5. You can exfiltrate known data to and from controlled hosts.

"""

# This is how we tell the LLM a template to give us back the actions (order 2 in sending)
EXAMPLE_PROMPT = """
Example of a state after an action:
- Known networks are 1.1.1.0/24
- Known hosts are 2.2.2.3
- Controlled hosts are 2.2.2.2 and 1.1.1.2
- Known data for source host 1.1.1.2: are (User1, WebData)
- Known services for host 1.1.1.1 are openssh

Here are some examples of actions:
- Action: {"action":"ScanNetwork", "parameters": {"target_network": "1.1.1.0/24"}}
- Action: {"action":"ScanServices", "parameters":{"target_host":"2.2.2.3"}}
- Action: {"action":"ExploitService", "parameters":{"target_host":"1.1.1.1", "target_service":"openssh"}}
- Action: {"action":"FindData", "parameters":{"target_host":"1.1.1.1"}}
- Action: {"action":"ExfiltrateData", "parameters": {"target_host": "2.2.2.2", "data": ("User1", "WebData"), "source_host": "1.1.1.2"}}}
End of examples.

"""

def get_long_term_interepisode_memory(actions_took_in_episode: list, type_of_end: str) -> str:
    """
    Build a prompt for the next episode
    Use the past list of actions and how the last game ended and
    ask the LLM to summarize it.
    """
    reward_memory = ''

    # TODO: Ask the LLM to summarize the episode.

    if type_of_end == 'win':
        reward_memory += f'\nYou won the last game with this action: {actions_took_in_episode[-1]}! Congratulations. Remember it.\n'
    elif type_of_end == 'detection':
        reward_memory += f'\nYou lost the last game because you were detected by the defender. Remember this.\n'
    elif type_of_end == 'max_steps':
        reward_memory += f'\nYou lost the last game because you did too many actions without reaching the goal. Remember this.\n'
    return reward_memory

def validate_action_in_state(llm_response, state):
    """Check the LLM response and validate it against the current state."""
    contr_hosts = [str(host) for host in state.controlled_hosts]
    known_hosts = [str(host) for host in state.known_hosts]
    known_nets = [str(net) for net in list(state.known_networks)]

    valid = False
    try:
        action_str = llm_response["action"]
        action_params = llm_response["parameters"]
        if isinstance(action_params, str):
            action_params = eval(action_params)
        match action_str:
            case 'ScanNetwork':
                if action_params["target_network"] in known_nets:
                    valid = True       
            case 'ScanServices':
                if action_params["target_host"] in known_hosts:
                    valid = True
            case 'ExploitService':
                ip_addr = action_params["target_host"]
                if ip_addr in known_hosts:
                    for service in state.known_services[IP(ip_addr)]:
                        if service.name == action_params["target_service"]:
                            valid = True
            case 'FindData':
                if action_params["target_host"] in contr_hosts:
                    valid = True
            case 'ExfiltrateData':
                for ip_data in state.known_data:
                    ip_addr = action_params["source_host"]
                    if ip_data == IP(ip_addr) and ip_addr in contr_hosts:
                        valid = True
            case _:
                valid = False
        return valid
    except:
        logger.info("Exception during validation of %s", llm_response)
        return False

def create_status_from_state(state, memory_list):
    """Create a status prompt using the current state and the sae memories."""
    contr_hosts = [host.ip for host in state.controlled_hosts]
    known_hosts = [host.ip for host in state.known_hosts if host.ip not in contr_hosts]
    known_nets = [str(net) for net in list(state.known_networks)]

    prompt = ''

    if len(memory_list) > 0:
        prompt = "List of past actions:\n"
        for memory in memory_list:
            #prompt += f'- Action {memory[0]} of {memory[1]}. {memory[2]}.\n'
            prompt += f'- Action {memory[0]}. {memory[1]}.\n'
        prompt += "End of list of past actions.\n\n"

    prompt += "Current status:\n"
    prompt += f"- Controlled hosts: {' and '.join(contr_hosts)}\n"
    logger.info("- Controlled hosts: %s", ' and '.join(contr_hosts))

    prompt += f"- Known networks: {' and '.join(known_nets)}\n"
    logger.info("Known networks: %s", ' and '.join(known_nets))

    if len(known_hosts) > 0:
        prompt += f"- Known hosts: {' and '.join(known_hosts)}\n"
        logger.info("- Known hosts: %s", ' and '.join(known_hosts))

    if len(state.known_services.keys()) == 0:
        prompt += "- Known services: None\n"
        logger.info(f"- Known services: None")
    for ip_service in state.known_services:
        services = []
        if len(list(state.known_services[ip_service])) > 0:
            for serv in state.known_services[ip_service]:
                if serv.name not in local_services_to_ignore:
                    services.append(serv.name)
            if len(services) > 0:
                serv_str = ""
                for serv in services:
                    serv_str += serv + " and "
                # Delete the last 'and '
                serv_str = serv_str[:-4]
                prompt += f"- Known services for host {ip_service}: {serv_str}\n"
                logger.info(f"- Known services for host {ip_service}: {services}")
            else:
                prompt += "- Known services: None\n"
                logger.info(f"- Known services: None")

    if len(state.known_data.keys()) == 0:
        prompt += "- Known data: None\n"
        logger.info(f"- Known data: None")
    for ip_data in state.known_data:
        if len(state.known_data[ip_data]) > 0:

            host_data = ""
            for known_data in list(state.known_data[ip_data]):
                host_data = f"({known_data.owner}, {known_data.id})"
                prompt += f"- Known data for host {ip_data}: {host_data}\n"
                logger.info(f"- Known data for host {ip_data}: {host_data}")

    prompt += "End of current status.\n"
    return prompt

def create_action_from_response(llm_response, state, actions_took_in_episode):
    """Build the action object from the llm response"""
    try:
        # Validate action based on current states
        valid = validate_action_in_state(llm_response, observation.state)
        action = None
        action_str = llm_response["action"]
        action_params = llm_response["parameters"]
        if isinstance(action_params, str):
            action_params = eval(action_params)
        if valid:
            match action_str:
                case 'ScanNetwork':
                    target_net, mask = action_params["target_network"].split('/')
                    action  = Action(ActionType.ScanNetwork, {"target_network":Network(target_net, int(mask))})
                case 'ScanServices':
                    action  = Action(ActionType.FindServices, {"target_host":IP(action_params["target_host"])})
                case 'ExploitService':
                    target_ip = action_params["target_host"]
                    target_service = action_params["target_service"]
                    if len(list(state.known_services[IP(target_ip)])) > 0:
                        for serv in state.known_services[IP(target_ip)]:
                            if serv.name == target_service:
                                parameters = {"target_host":IP(target_ip), "target_service":Service(serv.name, serv.type, serv.version, serv.is_local)}
                                action = Action(ActionType.ExploitService, parameters)
                case 'FindData':
                    action = Action(ActionType.FindData, {"target_host":IP(action_params["target_host"])})
                case 'ExfiltrateData':
                    data_owner, data_id = action_params["data"]
                    action = Action(ActionType.ExfiltrateData, {"target_host":IP(action_params["target_host"]), "data":Data(data_owner, data_id), "source_host":IP(action_params["source_host"])})
                case _:
                    return False, action

    except SyntaxError:
        logger.error(f"Cannol parse the response from the LLM: {llm_response}")
        valid = False

    if args.force_ignore:
        # Ignore action if it was taken before
        for past_action in actions_took_in_episode:
            if action == past_action:
                return False, False, actions_took_in_episode

    return valid, action

@retry(stop=stop_after_attempt(30))
def openai_query(msg_list, model, delay: float, max_tokens=60):
    """
    Send messages to OpenAI API and return the response.
    """
    time.sleep(delay)
    logger.info(f'Asking the openAI API')
    llm_response = openai.ChatCompletion.create(
        model=model,
        messages=msg_list,
        max_tokens=max_tokens,
        temperature=0.0
    )
    # We expect the response from the LLM to be JSON
    return llm_response["choices"][0]["message"]["content"]

if __name__ == "__main__":

    logger = logging.getLogger('llm')

    parser = argparse.ArgumentParser()
    parser.add_argument("--task_config_file", help="Reads the task definition from a configuration file", default=path.join(path.dirname(__file__), 'netsecenv-tests_02.yaml'), action='store', required=False)
    parser.add_argument("--test_episodes", help="Number of test episodes to run", default=30, action='store', required=False, type=int)
    parser.add_argument("--memory_buffer", help="Number of actions to remember and pass to the LLM", default=10, action='store', required=False, type=int)
    parser.add_argument("--llm", type=str, choices=["gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-0613", "gpt-3.5-turbo-0301"], default="gpt-3.5-turbo", help="LLM used with OpenAI API")
    parser.add_argument("--force_ignore", help="Force ignore repeated actions in code", default=False, action=argparse.BooleanOptionalAction)
    parser.add_argument("--long_memory", help="Remember between consecutive episodes.", default=False, action=argparse.BooleanOptionalAction)
    parser.add_argument("--delay", help="Delay the requests to LLM by this amount of seconds.", type=float, default=0)

    args = parser.parse_args()

    # Create the environment
    env = NetworkSecurityEnvironment(args.task_config_file)

    # Setup tensorboard
    run_name = f"netsecgame__llm__{env.seed}__{int(time.time())}"
    writer = SummaryWriter(f"agents/llm/logs/{run_name}")

    # Run multiple episodes to compute statistics
    wins = 0
    detected = 0
    reach_max_steps = 0
    returns = []
    num_steps = []
    num_win_steps = []
    num_detected_steps = []
    num_repeated_actions = []
    reward_memory = ''
    # We are still not using this, but we keep track
    is_detected = False

    # Control to save the 1st prompt in tensorboard
    save_first_prompt = False

    for episode in range(1, args.test_episodes + 1):

        actions_took_in_episode = []

        logger.info(f'Running episode {episode}')
        print(f'Running episode {episode}')

        observation = env.reset()
        current_state = observation.state

        # num_iterations is the max number of times we can ask the LLM to make 1 step. 
        # It is not the number of steps because many actions from the LLM are discarded.
        # All these iterations are for 1 episodes
        num_iterations = env._max_steps + 20
        taken_action = None
        memories = []
        total_reward = 0
        num_actions = 0
        number_repeated_actions_episode = 0

        # Populate the instructions based on the pre-defined goal
        # We do a deepcopy because when we later pop() the data will be also deleted in the env. Deepcopy avoids that.
        goal = copy.deepcopy(env._win_conditions)
        # Create the template to send to the llm
        jinja_environment = jinja2.Environment()
        template = jinja_environment.from_string(INSTRUCTIONS_TEMPLATE)
        # For now we know where to exfiltrate. We can put later 'to the public ip'
        target_host = list(goal["known_data"].keys())[0]
        data = goal["known_data"][target_host].pop()
        # Fill the instructions template with some info fromt the goal
        instructions = template.render(target_host=target_host)

        for i in range(num_iterations):
            # A good action is when the state changed after taking it. 
            # This is an estimation about if the action was succesfully executed. 
            # It is not precise because the action can be successful and still not change the state.
            good_action = False

            # Here memories are also added to the future prompt.
            status_prompt = create_status_from_state(observation.state, memories[-args.memory_buffer:])
            messages = [
                    {"role": "system", "content": instructions},
                    {"role": "user", "content": EXAMPLE_PROMPT},
                    {"role": "user", "content": reward_memory},
                    {"role": "user", "content": status_prompt},
                    {"role": "user", "content": "\nIf an action is in your list of past actions do not chose that same action!"},
                    {"role": "user", "content": "\nDO NOT REPEAT PAST ACTIONS!"},
                    {"role": "user", "content": "\nSelect a valid action with the correct format and parameters"},
                    {"role": "user", "content": "\nBefore answering check that the action you answer is not in the list of past actions"},
                    {"role": "user", "content": "\n\nAction: "}
                ]

            # Log the text to the LLM
            txt_message = ''
            for line in messages:
                content_temp = line['content'] 
                txt_message += content_temp.replace('\\n', '\n')

            logger.info(f'Text sent to the LLM: {txt_message}')

            # Store the first prompt in tensorboard
            if not save_first_prompt:
                writer.add_text('prompt', f'{messages}')
                save_first_prompt = True

            print(status_prompt)
            logger.info(status_prompt)
            # Query the LLM
            response = openai_query(messages, args.llm, args.delay)
            logger.info(f"Action chosen (not still taken) by LLM: {response}")
            print(f"Action chosen (not still taken) by LLM: {response}")

            try:
                # Since the response should be JSON, we can eval it and crate a dict
                response = eval(response)
                is_valid, action = create_action_from_response(response, observation.state, actions_took_in_episode)
            except Exception as e:
                # Some error happened?
                logger.info(f"Error while creating action from response: {e}")
                is_valid = False

            logger.info(f"Iteration: {i}. Is action valid to be taken: {is_valid}, did action change status: {good_action}")

            if is_valid:
                # Take action
                logger.info(f"Action taken: {response}")
                print(f"Action taken: {response}")
                observation = env.step(action)
                total_reward += observation.reward
                if observation.state != current_state:
                    good_action = True
                    current_state = observation.state

            logger.info(f"Did action change status: {good_action}")

            if observation.done or i == (num_iterations-1): # if it is the last iteration gather statistics
                if i < (num_iterations-1):
                    reason = observation.info
                else:
                    reason= {'end_reason' : 'max_iterations'}

                # Did we win?
                win = 0
                if observation.reward > 0:
                    win = 1
                # is_detected if boolean
                is_detected = env.detected
                steps = env.timestamp
                epi_last_reward = observation.reward
                if 'goal_reached' in reason['end_reason']:
                    wins += 1
                    num_win_steps += [steps]
                    type_of_end = 'win'
                elif 'detected' in reason['end_reason']:
                    detected += 1
                    num_detected_steps += [steps]
                    type_of_end = 'detection'
                elif 'max_iterations' in reason['end_reason']:
                    reach_max_steps += 1
                    type_of_end = 'max_iterations'
                    total_reward = -env._max_steps
                    steps = env._max_steps
                else:
                    reach_max_steps += 1
                    type_of_end = 'max_steps'
                # Store the number of repeated actions in this episode
                num_repeated_actions += [number_repeated_actions_episode]
                
                # Build the interepisode memory
                if args.long_memory:
                    reward_memory = get_long_term_interepisode_memory(actions_took_in_episode, type_of_end)
                    
                returns += [total_reward]
                num_steps += [steps]
                logger.info(f"\tEpisode {episode} of game ended after {steps} steps. Reason: {reason}. Last reward: {epi_last_reward}")
                print(f"\tEpisode {episode} of game ended after {steps} steps. Reason: {reason}. Last reward: {epi_last_reward}")
                # If the action was detected, then the episode is done
                break

            # Create the memory for the next step of the LLM
            try:
                # Stores a text for each memory
                memory_text = ''
                if not is_valid:
                    memory_text = "Action not valid in this state."
                else:
                    # This is based on the assumption that more valid actions in the state are better/more helpful.
                    # But we could a manual evaluation based on the prior knowledge and weight the different components.
                    # For example: finding new data is better than discovering hosts (?)
                    if good_action:
                        memory_text = 'This action found new information. '
                    else:
                        memory_text = "This action did not find new information. "

                    # If the action was repeated, criticize in prompt
                    for past_action in actions_took_in_episode:
                        if action == past_action:
                            memory_text += "This action you choose was in the list of past actions. I told you not to repeat actions from the list of past actions! "
                            number_repeated_actions_episode += 1
                            break
                    # Store action in memory of all actions so far 
                    actions_took_in_episode.append(action)

            except TypeError:
                # if the LLM sends a response that is not properly formatted.
                memory_text = " Action has bad format. Go back to create good formated actions."

            #memories.append((response["action"], response["parameters"], memory_text))
            memories.append((response, memory_text))

    # After all episodes are done. Compute statistics
    test_win_rate = (wins/(args.test_episodes))*100
    test_detection_rate = (detected/(args.test_episodes))*100
    test_max_steps_rate = (reach_max_steps/(args.test_episodes))*100
    test_average_returns = np.mean(returns)
    test_std_returns = np.std(returns)
    test_average_episode_steps = np.mean(num_steps)
    test_std_episode_steps = np.std(num_steps)
    test_average_win_steps = np.mean(num_win_steps)
    test_std_win_steps = np.std(num_win_steps)
    test_average_detected_steps = np.mean(num_detected_steps)
    test_std_detected_steps = np.std(num_detected_steps)
    test_average_repeated_actions = np.mean(num_repeated_actions)
    test_std_repeated_actions = np.std(num_repeated_actions)
    # Store in tensorboard
    tensorboard_dict = {"charts/test_avg_win_rate": test_win_rate, "charts/test_avg_detection_rate": test_detection_rate, "charts/test_avg_max_steps_rate": test_max_steps_rate, "charts/test_avg_returns": test_average_returns, "charts/test_std_returns": test_std_returns, "charts/test_avg_episode_steps": test_average_episode_steps, "charts/test_std_episode_steps": test_std_episode_steps, "charts/test_avg_win_steps": test_average_win_steps, "charts/test_std_win_steps": test_std_win_steps, "charts/test_avg_detected_steps": test_average_detected_steps, "charts/test_std_detected_steps": test_std_detected_steps, "charts/test_average_repeated_actions": test_average_repeated_actions, "charts/test_std_repeated_actions": test_std_repeated_actions}

    text = f'''Final test after {args.test_episodes} episodes
        Wins={wins},
        Detections={detected},
        winrate={test_win_rate:.3f}%,
        detection_rate={test_detection_rate:.3f}%,
        max_steps_rate={test_max_steps_rate:.3f}%,
        average_returns={test_average_returns:.3f} +- {test_std_returns:.3f},
        average_episode_steps={test_average_episode_steps:.3f} +- {test_std_episode_steps:.3f},
        average_win_steps={test_average_win_steps:.3f} +- {test_std_win_steps:.3f},
        average_detected_steps={test_average_detected_steps:.3f} +- {test_std_detected_steps:.3f}
        average_repeated actions={test_average_repeated_actions:.3f} +- {test_std_repeated_actions:.3f}
        '''

    # Text that is going to be added to the tensorboard. Put any description you want

    experiment_description = "LLM agent. " + f"Model: {args.llm}" + f"Conf: {args.task_config_file}" + f"Max steps: {env._max_steps}" + f"Seed: {env._seed}"

    writer.add_text("Description", experiment_description)
    writer.add_hparams(vars(args), tensorboard_dict)
    print(text)
    logger.info(text)
